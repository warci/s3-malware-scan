from time import sleep
import boto3
import json
import os
import logging
from botocore.exceptions import ClientError
import requests

class MalwareScan(object):
    def __init__(self) -> None:
        # Set up vars
        self.aws_region = os.environ.get('AWS_REGION', os.environ['AWS_DEFAULT_REGION'])
        self.malware_scan_queue_url = os.environ['MALWARE_SCAN_QUEUE_URL']
        self.quarantine_bucket = os.environ['QUARANTINE_BUCKET']
        self.clean_bucket = os.environ['CLEAN_BUCKET']
        self.sns_topic_arn = os.environ['SNS_TOPIC_ARN']
        self.environment_name = os.environ['ENVIRONMENT_NAME']
        self.environment_id = os.environ['ENVIRONMENT_ID']
        self.app_name = os.environ.get('APP_NAME', 'malware')
        self.temp_folder="/tmp"
        # Set up AWS clients
        self.sqs_client = boto3.client("sqs", region_name=self.aws_region)
        self.s3_client = boto3.client("s3", region_name=self.aws_region)
        self.sns_client = boto3.client("sns", region_name=self.aws_region)
        self.cw_client = boto3.client("cloudwatch", region_name=self.aws_region)
        # Set up logging
        self.cloudwatch_namespace = f"{self.environment_name}-{self.environment_id}-{self.app_name}"
        debug_enabled = os.environ.get('DEBUG', 1)
        if debug_enabled == 1:
            log_level = logging.DEBUG
        else:
            log_level = logging.INFO

        self.logger = logging.getLogger('Container logger')
        self.logger.setLevel(log_level)
        ch = logging.StreamHandler()
        ch.setLevel(log_level)
        formatter = logging.Formatter('%(levelname)s - %(message)s')
        ch.setFormatter(formatter)
        self.logger.addHandler(ch)

        self.logger.info("Listening for new files")

    def enable_task_termination_protection(self, protection_status: bool) -> None:
        url = f"{os.environ['ECS_AGENT_URI']}/task-protection/v1/state"
        r = requests.put(url, params={'ProtectionEnabled': protection_status})
        self.logger.debug(r.content)

    def put_metric_data(self, namespace, name, value, unit) -> None:
        """
        Sends a single data value to CloudWatch for a metric. This metric is given
        a timestamp of the current UTC time.
        :param namespace: The namespace of the metric.
        :param name: The name of the metric.
        :param value: The value of the metric.
        :param unit: The unit of the metric.
        """ 
        try:
            self.cw_client.put_metric_data(
                Namespace=namespace,
                MetricData=[{
                    'MetricName': name,
                    'Value': value,
                    'Unit': unit
                }]
            )
            self.logger.info("Put data for metric %s.%s", namespace, name)
        except ClientError:
            self.logger.exception("Couldn't put data for metric %s.%s", namespace, name)
            raise

    def publish_to_sns(self, sub, msg) -> None:
        self.logger.debug(f"Topic ARN: {os.environ['SNS_TOPIC_ARN']}")
        try:
            response = self.sns_client.publish(
                TopicArn=self.sns_topic_arn,
                Message=msg,
                Subject=sub
            )
        except ClientError:
            self.logger.exception(f"Couldn't send sns notification regarding: {sub}")
            raise

    def upload_to_destination(self, key, clean) -> None:
        if clean:
            bucket = self.clean_bucket
        else:
            bucket = self.quarantine_bucket

        self.logger.info(f"Copying {key} to {bucket}")   
        self.s3_client.upload_file(os.path.join(self.temp_folder, key), bucket, key)

        #TODO implement checksum
        #s3_checksum = S3.get_key(file_name).etag[1:-1]
        #local_checksum = find_checksum(file_name)

        #if s3_checksum != local_checksum:

    def copy_from_s3(self, bucket, key) -> None:
        self.logger.info(f"Downloading {key} from {bucket}")
        self.s3_client.download_file(
            bucket,
            key,
            os.path.join(self.temp_folder, key)
        )

    def delete_file_remote(self, bucket, key) -> None:
        self.logger.info(f"Purging local copy of {key}")
        #TODO: add S3 deletion code when done testing

    def delete_file_local(self, filename) -> None:
        self.logger.info(f"Deleting file {filename} from local disk")
        os.remove(os.path.join(self.temp_folder, filename))

    def scan_file(self, filename) -> bool:
        self.logger.info(f"Scanning {filename}")
        self.enable_task_termination_protection(True)
        sleep(5)
        #TODO: run actual scan
        # return true if clean, false if file needs to be quarantined
        self.enable_task_termination_protection(False)
        retval=True

        return retval

    def receive_messages(self, queue_url, max_number_of_messages = 1, wait_timeout_seconds = 10) -> list:
        self.logger.info("Retreiving messages from queue")
        response = self.sqs_client.receive_message(
            QueueUrl=queue_url,
            MaxNumberOfMessages=max_number_of_messages,
            WaitTimeSeconds=wait_timeout_seconds,
        )

        if response.get("Messages", None):
            self.logger.debug(f"Number of messages received: {len(response.get('Messages', []))}")
        
        return response.get("Messages", None)

    def delete_message(self, queue_url, receipt_handle) -> None:
        self.logger.info(f"Deleting message from queue")
        response = self.sqs_client.delete_message(
            QueueUrl=queue_url,
            ReceiptHandle=receipt_handle,
    )

    def do_scan(self, message) -> None:
        message_body = json.loads(message["Body"])
        key = message_body['detail']['object']['key']
        bucket = message_body['detail']['bucket']['name']
        receipt_handle = message["ReceiptHandle"]
        
        try:
            self.copy_from_s3(bucket, key)
            clean = self.scan_file(key)
                            
            if clean:
                self.logger.info(f"File {key} is clean")
                self.put_metric_data(self.cloudwatch_namespace, 'files_clean', 1, 'Count')
            else:
                self.logger.info(f"File {key} contains malware!")
                self.put_metric_data(self.cloudwatch_namespace, 'files_infected', 1, 'Count')

            self.upload_to_destination( key, clean)
            self.delete_file_local(key)
            self.delete_file_remote( bucket, key)
            self.delete_message(self.malware_scan_queue_url, receipt_handle)
            self.logger.info(f"Processing {key} from bucket {bucket} done.")
        except Exception as e:
            self.publish_to_sns(
                "Malware scanning failed",
                f"Error while scanning {key} from S3 bucket {bucket}, please investigate."
            )
            raise e


if __name__ == '__main__':
    for var in ["MALWARE_SCAN_QUEUE_URL", 'QUARANTINE_BUCKET', 'CLEAN_BUCKET', 'SNS_TOPIC_ARN', 'ENVIRONMENT_NAME', 'ENVIRONMENT_ID']:
        if var not in os.environ:
            raise EnvironmentError("Please set environment variable {}.".format(var))

    malware_scan = MalwareScan()
    
    while True:
        messages = malware_scan.receive_messages(malware_scan.malware_scan_queue_url)
        if messages != None:
            for message in messages:
                malware_scan.do_scan(message)  
        else:
            sleep(5)